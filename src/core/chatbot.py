from .embedding import get_embedding_model
from .faiss_manager import load_faiss_index
from langchain_core.prompts import ChatPromptTemplate
from langchain_mistralai.chat_models import ChatMistralAI
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

def get_retriever(embedding_model, index_path="data/faiss_index"):
    """
    CrÃ©e et retourne un retriever Ã  partir d'un index FAISS existant.
    """
    # Charger la base de donnÃ©es vectorielle
    vectorstore = load_faiss_index(embedding_model, index_path)
    
    # Transformer la base de donnÃ©es en un "retriever"
    # search_kwargs={'k': 3} signifie qu'on rÃ©cupÃ©rera les 3 chunks les plus pertinents.
    return vectorstore.as_retriever(search_kwargs={'k': 5})


def create_prompt_template():
    """
    CrÃ©e et retourne un template de prompt pour le chatbot RAG.
    """
    template = """
    Tu es un assistant spÃ©cialisÃ© dans la recommandation d'Ã©vÃ©nements publics.
    RÃ©ponds Ã  la question de l'utilisateur en te basant uniquement sur le contexte suivant.
    Sois aimable, concis et prÃ©sente les informations de maniÃ¨re claire, par exemple avec des listes Ã  puces.
    Si le contexte ne contient pas la rÃ©ponse, dis simplement que tu n'as pas trouvÃ© d'information Ã  ce sujet.

    Contexte :
    {context}

    Question :
    {question}

    RÃ©ponse :
    """
    return ChatPromptTemplate.from_template(template)


def create_rag_chain(retriever, prompt, embedding_model):
    """
    CrÃ©e et retourne une chaÃ®ne RAG complÃ¨te.
    """
    # Initialiser le modÃ¨le de chat Mistral
    llm = ChatMistralAI(
        model="open-mistral-7b",
        temperature=0.1, # Peu de crÃ©ativitÃ© pour s'en tenir aux faits
        api_key=embedding_model.mistral_api_key # On rÃ©utilise la clÃ©
    )
    
    # Fonction pour formater les documents rÃ©cupÃ©rÃ©s
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    # CrÃ©ation de la chaÃ®ne RAG avec la syntaxe LCEL
    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    
    return rag_chain


if __name__ == '__main__':
    # 1. Initialiser le modÃ¨le d'embedding (nÃ©cessaire pour le retriever)
    embedding_model = get_embedding_model()
    
    # 2. CrÃ©er le retriever
    retriever = get_retriever(embedding_model)
    
    # 3. CrÃ©er le prompt
    prompt = create_prompt_template()
    
    # 4. CrÃ©er la chaÃ®ne RAG
    rag_chain = create_rag_chain(retriever, prompt, embedding_model)
    
    print("ğŸ¤– Chatbot d'Ã©vÃ©nements prÃªt ! Posez vos questions (tapez 'exit' pour quitter).")
    
    while True:
        query = input("Vous > ")
        if query.lower() == 'exit':
            break
            
        # 5. Invoquer la chaÃ®ne pour obtenir une rÃ©ponse
        response = rag_chain.invoke(query)
        
        print(f"Bot > {response}")